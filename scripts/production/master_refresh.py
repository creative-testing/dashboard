#!/usr/bin/env python3
"""
SCRIPT MASTER INTELLIGENT
Refresh toutes les p√©riodes avec date de r√©f√©rence dynamique (toujours hier)
Solution d√©finitive pour coh√©rence des donn√©es
"""
import os
import requests
import json
import sys
import subprocess
import shutil
from glob import glob
from dotenv import load_dotenv
from datetime import datetime, timedelta
from collections import defaultdict
import time as _time
from concurrent.futures import ThreadPoolExecutor, as_completed

load_dotenv()

def get_reference_date():
    """Date de r√©f√©rence intelligente : toujours hier (journ√©e compl√®te)"""
    yesterday = datetime.now() - timedelta(days=1)
    return yesterday.strftime('%Y-%m-%d')

def calculate_period_dates(period_days, reference_date):
    """Calcule fen√™tre pour une p√©riode depuis date r√©f√©rence"""
    ref = datetime.strptime(reference_date, '%Y-%m-%d')
    end_date = ref
    start_date = ref - timedelta(days=period_days - 1)
    
    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')

def fetch_account_insights_optimized(account, token, since_date, until_date):
    """Fetch optimis√© pour un compte"""
    account_id = account["id"]
    account_name = account.get("name", "Sans nom")
    
    try:
        url = f"https://graph.facebook.com/v23.0/{account_id}/insights"
        params = {
            "access_token": token,
            "level": "ad",
            "time_range": f'{{"since":"{since_date}","until":"{until_date}"}}',
            "time_increment": 1,
            "fields": "ad_id,ad_name,campaign_name,adset_name,impressions,spend,clicks,ctr,cpm,reach,frequency,actions,action_values,cost_per_action_type",
            "filtering": '[{"field":"impressions","operator":"GREATER_THAN","value":"0"}]',
            "limit": 1000
        }
        
        # Pagination compl√®te
        all_ads = []
        current_url = url
        page = 0
        
        backoff = 1
        while current_url and page < 30:  # Limite s√©curit√©
            try:
                if page == 0:
                    response = requests.get(current_url, params=params)
                else:
                    response = requests.get(current_url)
                try:
                    data = response.json()
                except Exception:
                    data = {}
                # rate limit handling
                if response.status_code != 200:
                    msg = str(data)
                    if '#80004' in msg or 'too many calls' in msg.lower() or response.status_code == 429:
                        import time, random
                        delay = min(30, backoff) + random.random()
                        time.sleep(delay)
                        backoff *= 2
                        continue
                    break
            except Exception:
                break
            
            if "data" in data:
                ads = data["data"]
                
                # Enrichir avec account info
                for ad in ads:
                    ad['account_name'] = account_name
                    ad['account_id'] = account_id
                
                all_ads.extend(ads)
                
                # Pagination
                if "paging" in data and "next" in data["paging"]:
                    current_url = data["paging"]["next"]
                    page += 1
                else:
                    break
            else:
                break
        
        return all_ads
        
    except Exception as e:
        return []

def fetch_creatives_parallel(ad_ids, token):
    """Fetch creatives en parall√®le"""
    
    def fetch_batch(ad_ids_batch):
        batch_requests = [
            {"method": "GET", "relative_url": f"{ad_id}?fields=creative{{video_id,image_url,instagram_permalink_url}}"}
            for ad_id in ad_ids_batch
        ]
        
        try:
            response = requests.post("https://graph.facebook.com/v23.0/", data={
                "access_token": token,
                "batch": json.dumps(batch_requests)
            })
            
            results = response.json()
            creatives = {}
            
            # ‚úÖ Fix: V√©rifier que results est une liste
            if not isinstance(results, list):
                print(f"‚ö†Ô∏è  Batch response not a list: {type(results)}")
                return {}
            
            for result in results:
                # ‚úÖ Fix: V√©rifier que result est un dict  
                if not isinstance(result, dict):
                    continue
                    
                if result.get("code") == 200:
                    body = json.loads(result["body"])
                    ad_id = body.get("id")
                    if ad_id and "creative" in body:
                        creatives[ad_id] = body["creative"]
            
            return creatives
            
        except:
            return {}
    
    # Diviser en batches
    batch_size = 100
    batches = [ad_ids[i:i+batch_size] for i in range(0, len(ad_ids), batch_size)]
    
    all_creatives = {}
    
    with ThreadPoolExecutor(max_workers=30) as executor:  # Agressif pour M1 Pro
        futures = [executor.submit(fetch_batch, batch) for batch in batches]
        
        for future in as_completed(futures):
            batch_creatives = future.result()
            all_creatives.update(batch_creatives)
    
    return all_creatives

def master_refresh():
    """Fonction master : refresh tout avec coh√©rence"""
    
    token = os.getenv("FACEBOOK_ACCESS_TOKEN")
    if not token:
        raise SystemExit("FACEBOOK_ACCESS_TOKEN not set. Define it in .env")
    reference_date = get_reference_date()
    
    print("üöÄ MASTER REFRESH - SOLUTION INTELLIGENTE")
    print("=" * 70)
    print(f"üìÖ Date r√©f√©rence (hier): {reference_date}")
    print(f"üïê Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üíª Optimis√© pour MacBook M1 Pro")
    
    start_master = _time.time()
    
    # 1. Comptes (une seule fois)
    print(f"\nüìä R√©cup√©ration comptes...")
    # Adaccounts pagination
    accounts = []
    url = "https://graph.facebook.com/v23.0/me/adaccounts"
    params = {
        "access_token": token,
        "fields": "id,name,account_status",
        "limit": 100,
    }
    retry = 0
    while url:
        resp = requests.get(url, params=params)
        try:
            data = resp.json()
        except Exception:
            data = {}
        if resp.status_code != 200:
            print(f"‚ö†Ô∏è adaccounts HTTP {resp.status_code}: {str(data)[:200]}")
            # simple backoff on rate limits
            if data and isinstance(data, dict) and 'error' in data:
                msg = str(data['error'])
                if '#80004' in msg or 'too many calls' in msg.lower():
                    import time
                    delay = min(30, 2 ** retry)
                    print(f"‚è≥ Rate limit: attente {delay}s et nouvelle tentative...")
                    time.sleep(delay)
                    retry += 1
                    if retry <= 3:
                        continue
            break
        accounts.extend(data.get("data", []) or [])
        # follow next
        url = data.get("paging", {}).get("next")
        params = None  # next already has params

    active_accounts = [acc for acc in accounts if (acc.get("account_status") == 1 or acc.get("account_status") == "1")]
    
    # Debug (optional)
    if os.getenv('DEBUG_REFRESH'):
        print(f"üëÄ Adaccounts renvoy√©s: {len(accounts)}")
        if accounts[:3]:
            try:
                sample = [{k: a.get(k) for k in ("id","name","account_status")} for a in accounts[:3]]
                print(f"   √©chantillon: {sample}")
            except Exception:
                pass

    # Fallback: explicit account IDs via env if nothing returned
    if not active_accounts:
        env_ids = os.getenv("META_ACCOUNT_IDS", "").strip()
        if env_ids:
            print("‚ö†Ô∏è Aucun compte actif list√© via /me/adaccounts, utilisation de META_ACCOUNT_IDS")
            active_accounts = [{"id": acc.strip(), "name": acc.strip(), "account_status": 1} for acc in env_ids.split(',') if acc.strip()]
    print(f"‚úÖ {len(active_accounts)} comptes actifs")
    
    # 2. Mode baseline journalier 90j + agr√©gations locales (moins d'appels)
    results_summary = {}
    print("\nüß± Baseline journali√®re 90j (time_increment=1)...")
    start_baseline = _time.time()

    # Fen√™tre 90 jours
    since_date_90, until_date_90 = calculate_period_dates(90, reference_date)

    # Collecte journali√®re pour chaque compte
    daily_rows = []
    with ThreadPoolExecutor(max_workers=8) as executor:
        futs = [
            executor.submit(fetch_account_insights_optimized, acc, token, since_date_90, until_date_90)
            for acc in active_accounts
        ]
        for fut in as_completed(futs):
            daily_rows.extend(fut.result())

    # Taguer chaque ligne avec une date si non pr√©sente (Meta renvoie souvent date_start/date_stop ou des buckets)
    for r in daily_rows:
        # si disponible, garder la granularit√©/jour (Meta inclut souvent date_start/date_stop par bucket)
        r['date'] = r.get('date_start') or r.get('date') or ''

    baseline = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'reference_date': reference_date,
            'date_range': f"{since_date_90} to {until_date_90}",
            'method': 'baseline_90d_daily',
            'total_rows': len(daily_rows)
        },
        'daily_ads': daily_rows
    }
    os.makedirs('data/current', exist_ok=True)
    with open('data/current/baseline_90d_daily.json', 'w', encoding='utf-8') as f:
        json.dump(baseline, f, indent=2, ensure_ascii=False)
    print(f"   ‚úÖ Baseline 90j: {len(daily_rows)} lignes en {_time.time()-start_baseline:.1f}s")

    # Agr√©gations locales vers 3/7/14/30/90
    print("\nüßÆ Agr√©gation locale vers p√©riodes...")
    # Import local pour √©viter probl√®mes de chemins
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
    from utils.aggregate_periods import aggregate_from_baseline  # type: ignore
    periods = [3,7,14,30,90]
    for period in periods:
        out = aggregate_from_baseline('data/current/baseline_90d_daily.json', period_days=period, reference_date=reference_date)
        with open(f'data/current/hybrid_data_{period}d.json', 'w', encoding='utf-8') as f:
            json.dump(out, f, indent=2, ensure_ascii=False)
        results_summary[period] = {
            'ads': out['metadata'].get('total_ads', 0),
            'spend': sum(a.get('spend',0) for a in out.get('ads',[])),
            'time': 0.0,
        }
        print(f"   ‚úÖ {period}j: {results_summary[period]['ads']} ads agr√©g√©es")
    
    # R√©sum√© final
    master_time = _time.time() - start_master
    
    print(f"\n" + "=" * 70)
    print(f"üéâ MASTER REFRESH TERMIN√â")
    print(f"‚ö° Temps total: {master_time/60:.1f} minutes")
    print(f"üìÖ Toutes p√©riodes coh√©rentes depuis: {reference_date}")
    
    print(f"\nüìä R√âSUM√â PAR P√âRIODE:")
    for period, stats in results_summary.items():
        print(f"  {period:2}j: {stats['ads']:4} ads, ${stats['spend']:>8,.0f} MXN, {stats['time']:4.1f}s")
    
    # Cr√©er fichier de config pour l'interface
    config_output = {
        "last_update": datetime.now().isoformat(),
        "reference_date": reference_date,
        "periods_available": list(results_summary.keys()),
        "total_execution_time": master_time
    }
    
    with open('data/current/refresh_config.json', 'w') as f:
        json.dump(config_output, f, indent=2)
    
    print(f"\nüíæ Config sauvegard√©e pour interface")

    # Si aucune annonce n'a √©t√© r√©cup√©r√©e, √©viter d'√©craser des fichiers valides
    total_ads_all = sum(v.get('ads', 0) for v in results_summary.values())

    # 3. R√©cup√©ration semaine pr√©c√©dente (pour comparaison)
    try:
        print("\nüìÜ Refresh semaine pr√©c√©dente (comparaison)...")
        subprocess.run([sys.executable, 'scripts/production/fetch_prev_week.py'], check=True)
        print("‚úÖ Semaine pr√©c√©dente OK")
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de rafra√Æchir la semaine pr√©c√©dente: {e}")

    # 4. Enrichissement media_url (int√©gr√©)
    try:
        print("\nüé¨ Enrichissement media_url (int√©gr√©) sur data/current...")
        # Import paresseux pour √©viter probl√®mes de path
        sys.path.append(os.path.join(os.path.dirname(__file__), '..'))  # add scripts/
        from utils.enrich_media import enrich_media_urls_union  # type: ignore
        stats = enrich_media_urls_union(base_dir='data/current', periods=[3,7,14,30,90], max_workers=10, only_missing=True)
        print("‚úÖ Enrichissement media_url OK:")
        for k,v in stats.items():
            if k.startswith('_'): continue
            print(f"   {k}d: {v.get('with_media',0)}/{v.get('total',0)}")
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible d'enrichir media_url: {e}")

    # 5. Miroir de compatibilit√© vers la racine (source de v√©rit√© = data/current)
    mirror_flag = os.getenv('MIRROR_TO_ROOT', 'false').lower() in ('1','true','yes','on')
    if total_ads_all > 0 and mirror_flag:
        try:
            print("\nüîÅ Miroir des fichiers vers la racine (compatibilit√©)...")
            files = [
                *(f"data/current/hybrid_data_{p}d.json" for p in [3, 7, 14, 30, 90]),
                "data/current/hybrid_data_prev_week.json",
                "data/current/refresh_config.json",
            ]
            # Petcare (facultatif)
            petcare_json = 'data/current/petcare_parsed_analysis.json'
            if os.path.exists(petcare_json):
                files.append(petcare_json)

            for src in files:
                if os.path.exists(src):
                    dst = os.path.basename(src)
                    shutil.copy2(src, dst)
            print("‚úÖ Miroir racine OK")
        except Exception as e:
            print(f"‚ö†Ô∏è Miroir racine √©chou√©: {e}")
    else:
        if total_ads_all == 0:
            print("‚ö†Ô∏è 0 annonces r√©cup√©r√©es: pas de miroir vers la racine pour prot√©ger les fichiers existants.")
        elif not mirror_flag:
            print("‚ÑπÔ∏è MIRROR_TO_ROOT d√©sactiv√©: pas de copie des JSON vers la racine.")

    return results_summary

if __name__ == "__main__":
    print("ü§ñ SCRIPT MASTER - REFRESH INTELLIGENT")
    print("üéØ Toutes p√©riodes coh√©rentes depuis date r√©f√©rence dynamique")
    print("‚ö° Optimis√© MacBook M1 Pro - Parall√©lisation aggressive") 
    print()
    
    results = master_refresh()
    
    if results:
        print(f"\n‚úÖ SUCCESS! Toutes donn√©es coh√©rentes et pr√™tes")
        print(f"üéØ Interface peut maintenant afficher dates pr√©cises")
        print(f"üöÄ Dashboard Pablo ready!")
