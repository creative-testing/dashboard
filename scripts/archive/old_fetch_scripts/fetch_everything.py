#!/usr/bin/env python3
"""
FETCH EVERYTHING - Script complet unifi√©
R√©cup√®re TOUTES les donn√©es en un seul passage:
- Insights avec d√©mographies (age/gender breakdown)
- Status et format via creatives
- Media URLs construites
"""
import os
import requests
import json
import sys
from dotenv import load_dotenv
from datetime import datetime, timedelta
from collections import defaultdict
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

def get_reference_date():
    """Date de r√©f√©rence : hier"""
    yesterday = datetime.now() - timedelta(days=1)
    return yesterday.strftime('%Y-%m-%d')

def fetch_account_data_with_demographics(account, token, since_date, until_date):
    """R√©cup√©ration avec d√©mographies ET m√©triques"""
    account_id = account["id"]
    account_name = account.get("name", "Unknown")
    
    all_rows = []
    
    try:
        url = f"https://graph.facebook.com/v23.0/{account_id}/insights"
        
        params = {
            "access_token": token,
            "level": "ad",
            "time_range": f'{{"since":"{since_date}","until":"{until_date}"}}',
            "time_increment": 1,  # Journalier
            "fields": "ad_id,ad_name,campaign_name,campaign_id,adset_name,adset_id,impressions,spend,clicks,reach,frequency,actions,action_values,created_time",
            "breakdowns": "age,gender",  # NOUVEAU: d√©mographies!
            "filtering": json.dumps([{"field": "impressions", "operator": "GREATER_THAN", "value": 0}]),
            "limit": 500
        }
        
        current_url = url
        page = 0
        max_pages = 100
        
        while current_url and page < max_pages:
            # GROS TIMEOUT : 4 minutes par requ√™te (plus de donn√©es avec demographics)
            if page == 0:
                response = requests.get(current_url, params=params, timeout=240)
            else:
                response = requests.get(current_url, timeout=240)
            
            if response.status_code == 429:  # Rate limit
                logger.info(f"  Rate limit {account_name}, attente 30s...")
                time.sleep(30)
                continue
            
            if response.status_code != 200:
                logger.warning(f"  Erreur {response.status_code} pour {account_name}")
                break
            
            data = response.json()
            
            if "data" in data:
                rows_batch = data["data"]
                
                # Enrichir chaque ligne avec account info
                for row in rows_batch:
                    row['account_name'] = account_name
                    row['account_id'] = account_id
                    
                    # Les champs age et gender sont d√©j√† pr√©sents gr√¢ce au breakdown
                    # row['age'] et row['gender'] existent
                    
                    # Extraire purchases
                    purchases = 0
                    purchase_value = 0.0
                    
                    for action in row.get('actions', []):
                        if 'purchase' in action.get('action_type', ''):
                            purchases += int(action.get('value', 0))
                    
                    for value in row.get('action_values', []):
                        if 'purchase' in value.get('action_type', ''):
                            purchase_value += float(value.get('value', 0))
                    
                    row['purchases'] = purchases
                    row['purchase_value'] = purchase_value
                    
                    spend = float(row.get('spend', 0))
                    row['roas'] = purchase_value / spend if spend > 0 else 0
                    row['cpa'] = spend / purchases if purchases > 0 else 0
                    row['date'] = row.get('date_start', '')
                
                all_rows.extend(rows_batch)
                
                # Pagination
                if "paging" in data and "next" in data["paging"]:
                    current_url = data["paging"]["next"]
                    page += 1
                    time.sleep(0.3)  # Petit d√©lai entre pages
                else:
                    break
            else:
                break
                
        logger.info(f"‚úì {account_name[:30]}: {len(all_rows)} lignes (avec demo)")
        
    except requests.exceptions.Timeout:
        logger.warning(f"‚è±Ô∏è Timeout {account_name} apr√®s 2 minutes")
    except Exception as e:
        logger.error(f"‚ùå Erreur {account_name}: {str(e)[:50]}")
    
    return all_rows

def aggregate_demographic_data(rows_with_demographics):
    """Agr√®ge les donn√©es d√©mographiques par ad_id"""
    ads_aggregated = defaultdict(lambda: {
        'impressions': 0,
        'spend': 0,
        'clicks': 0,
        'reach': 0,
        'purchases': 0,
        'purchase_value': 0,
        'demographics_breakdown': []
    })
    
    for row in rows_with_demographics:
        ad_id = row.get('ad_id')
        if not ad_id:
            continue
            
        # Agr√©ger les m√©triques principales
        ad = ads_aggregated[ad_id]
        
        # Premi√®re fois qu'on voit cet ad_id? Copier les m√©tadonn√©es
        if 'ad_name' not in ad:
            ad['ad_id'] = ad_id
            ad['ad_name'] = row.get('ad_name')
            ad['campaign_name'] = row.get('campaign_name')
            ad['campaign_id'] = row.get('campaign_id')
            ad['adset_name'] = row.get('adset_name')
            ad['adset_id'] = row.get('adset_id')
            ad['account_name'] = row.get('account_name')
            ad['account_id'] = row.get('account_id')
            ad['created_time'] = row.get('created_time')
            ad['date'] = row.get('date')
        
        # Agr√©ger les m√©triques
        ad['impressions'] += int(row.get('impressions', 0))
        ad['spend'] += float(row.get('spend', 0))
        ad['clicks'] += int(row.get('clicks', 0))
        ad['reach'] += int(row.get('reach', 0))
        ad['purchases'] += int(row.get('purchases', 0))
        ad['purchase_value'] += float(row.get('purchase_value', 0))
        
        # Ajouter le d√©tail d√©mographique
        if row.get('age') and row.get('gender'):
            ad['demographics_breakdown'].append({
                'age': row.get('age'),
                'gender': row.get('gender'),
                'spend': float(row.get('spend', 0)),
                'impressions': int(row.get('impressions', 0)),
                'clicks': int(row.get('clicks', 0))
            })
    
    # Calculer les ratios finaux
    result = []
    for ad_id, ad in ads_aggregated.items():
        spend = ad['spend']
        purchases = ad['purchases']
        purchase_value = ad['purchase_value']
        
        ad['roas'] = purchase_value / spend if spend > 0 else 0
        ad['cpa'] = spend / purchases if purchases > 0 else 0
        ad['frequency'] = ad['impressions'] / ad['reach'] if ad['reach'] > 0 else 0
        
        result.append(ad)
    
    return result

def fetch_creatives_batch(ad_ids, token):
    """R√©cup√®re les creatives en batch pour status, format et media_url"""
    if not ad_ids:
        return {}
        
    creative_map = {}
    
    # Diviser en chunks de 50
    for i in range(0, len(ad_ids), 50):
        chunk = ad_ids[i:i+50]
        
        try:
            # Requ√™te batch
            batch_requests = [
                {
                    "method": "GET", 
                    "relative_url": f"{ad_id}?fields=status,effective_status,created_time,creative{{id,video_id,image_url,instagram_permalink_url,effective_object_story_id,object_story_id}}"
                }
                for ad_id in chunk
            ]
            
            response = requests.post(
                "https://graph.facebook.com/v23.0/",
                data={
                    "access_token": token,
                    "batch": json.dumps(batch_requests)
                },
                timeout=30
            )
            
            if response.status_code == 200:
                results = response.json()
                
                for idx, result in enumerate(results):
                    if isinstance(result, dict) and result.get("code") == 200:
                        body = json.loads(result.get("body", "{}"))
                        ad_id = body.get("id")
                        
                        if ad_id:
                            creative_info = {
                                'status': body.get('status'),
                                'effective_status': body.get('effective_status'),
                                'created_time': body.get('created_time')
                            }
                            
                            # D√©tection du format et construction de l'URL
                            creative = body.get('creative', {})
                            
                            if creative.get('video_id'):
                                creative_info['format'] = 'VIDEO'
                                creative_info['media_url'] = f"https://www.facebook.com/watch/?v={creative['video_id']}"
                            elif creative.get('image_url'):
                                creative_info['format'] = 'IMAGE'
                                creative_info['media_url'] = creative['image_url']
                            elif creative.get('instagram_permalink_url'):
                                creative_info['format'] = 'INSTAGRAM'
                                creative_info['media_url'] = creative['instagram_permalink_url']
                            else:
                                # Fallback to story permalink
                                story_id = creative.get('effective_object_story_id') or creative.get('object_story_id')
                                if story_id:
                                    # On pourrait r√©cup√©rer le permalink mais c'est plus complexe
                                    creative_info['format'] = 'POST'
                                    creative_info['story_id'] = story_id
                                else:
                                    creative_info['format'] = 'UNKNOWN'
                            
                            creative_map[ad_id] = creative_info
                            
        except Exception as e:
            logger.warning(f"Erreur batch creatives: {str(e)[:50]}")
            continue
            
        time.sleep(0.2)  # Petit d√©lai entre batches
    
    return creative_map

def enrich_with_creatives(ads_list, token):
    """Enrichit la liste des ads avec status, format et media_url"""
    logger.info("üé® Enrichissement avec creatives...")
    
    # Extraire tous les ad_ids uniques
    ad_ids = list(set(ad['ad_id'] for ad in ads_list if ad.get('ad_id')))
    logger.info(f"  {len(ad_ids)} ads uniques √† enrichir")
    
    # R√©cup√©rer les creatives en batch
    creative_map = fetch_creatives_batch(ad_ids, token)
    logger.info(f"  {len(creative_map)} creatives r√©cup√©r√©es")
    
    # Enrichir chaque ad
    enriched_count = 0
    for ad in ads_list:
        ad_id = ad.get('ad_id')
        if ad_id and ad_id in creative_map:
            creative_info = creative_map[ad_id]
            
            # Ajouter les champs
            ad['status'] = creative_info.get('status', 'UNKNOWN')
            ad['effective_status'] = creative_info.get('effective_status', 'UNKNOWN')
            ad['format'] = creative_info.get('format', 'UNKNOWN')
            
            if 'media_url' in creative_info:
                ad['media_url'] = creative_info['media_url']
            elif 'story_id' in creative_info:
                ad['media_url'] = f"https://www.facebook.com/{creative_info['story_id']}"
            
            enriched_count += 1
    
    logger.info(f"‚úÖ {enriched_count} ads enrichies avec creatives")
    return ads_list

def main():
    """Fonction principale COMPLETE"""
    print("üöÄ FETCH EVERYTHING - R√©cup√©ration compl√®te unifi√©e")
    print("=" * 70)
    
    token = os.getenv("FACEBOOK_ACCESS_TOKEN")
    if not token:
        logger.error("‚ùå Token non trouv√©")
        sys.exit(1)
    
    reference_date = get_reference_date()
    start_time = time.time()
    
    DAYS_TO_FETCH = int(os.getenv('FETCH_DAYS', '30'))
    print(f"üìÖ P√©riode: {DAYS_TO_FETCH} jours jusqu'√† {reference_date}")
    print(f"‚ö° Avec d√©mographies, status, format et media URLs")
    print(f"‚è±Ô∏è  Timeout: 4 minutes par requ√™te, 22 workers parall√®les")
    print(f"üíª MacBook M1 Pro 64GB - Optimis√© pour stabilit√©")
    
    # 1. R√©cup√©rer tous les comptes
    print("\nüìä R√©cup√©ration des comptes...")
    url = "https://graph.facebook.com/v23.0/me/adaccounts"
    params = {
        "access_token": token,
        "fields": "id,name,account_status",
        "limit": 200
    }
    
    response = requests.get(url, params=params, timeout=30)
    if response.status_code != 200:
        logger.error("Erreur r√©cup√©ration comptes")
        sys.exit(1)
    
    accounts = response.json().get("data", [])
    active_accounts = [acc for acc in accounts if acc.get("account_status") == 1]
    print(f"‚úÖ {len(active_accounts)} comptes actifs")
    
    # 2. Dates
    since_date = (datetime.strptime(reference_date, '%Y-%m-%d') - timedelta(days=DAYS_TO_FETCH-1)).strftime('%Y-%m-%d')
    until_date = reference_date
    
    print(f"\nüìà Phase 1: Insights avec d√©mographies {since_date} √† {until_date}")
    print(f"‚ö° Parall√©lisation x22 (√©quilibre optimal)")
    
    all_demographic_rows = []
    failed_accounts = []
    
    # 3. Phase 1: R√©cup√©ration avec d√©mographies
    with ThreadPoolExecutor(max_workers=22) as executor:
        futures = {
            executor.submit(fetch_account_data_with_demographics, account, token, since_date, until_date): account
            for account in active_accounts
        }
        
        completed = 0
        for future in as_completed(futures):
            account = futures[future]
            try:
                rows = future.result(timeout=180)  # 3 minutes max d'attente
                if rows:
                    all_demographic_rows.extend(rows)
                completed += 1
                
                if completed % 5 == 0:
                    print(f"  Progress: {completed}/{len(active_accounts)} comptes")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec {account.get('name', 'Unknown')}")
                failed_accounts.append(account.get('name', 'Unknown'))
                completed += 1
    
    print(f"\n‚úÖ Phase 1 termin√©e: {len(all_demographic_rows)} lignes d√©mographiques")
    
    # 4. Phase 2: Agr√©gation par ad_id
    print("\nüìä Phase 2: Agr√©gation des donn√©es...")
    aggregated_ads = aggregate_demographic_data(all_demographic_rows)
    print(f"‚úÖ {len(aggregated_ads)} ads uniques apr√®s agr√©gation")
    
    # 5. Phase 3: Enrichissement avec creatives
    print("\nüé® Phase 3: Enrichissement creatives (status, format, media)...")
    enriched_ads = enrich_with_creatives(aggregated_ads, token)
    
    # 6. Stats finales
    print(f"\nüìä Statistiques finales:")
    print(f"  Total ads: {len(enriched_ads)}")
    print(f"  Avec status: {sum(1 for ad in enriched_ads if ad.get('status'))}")
    print(f"  Avec format: {sum(1 for ad in enriched_ads if ad.get('format'))}")
    print(f"  Avec media_url: {sum(1 for ad in enriched_ads if ad.get('media_url'))}")
    print(f"  Avec d√©mographies: {sum(1 for ad in enriched_ads if ad.get('demographics_breakdown'))}")
    
    if failed_accounts:
        print(f"‚ö†Ô∏è {len(failed_accounts)} comptes ont √©chou√©: {', '.join(failed_accounts[:5])}")
    
    # 7. Sauvegarder les donn√©es
    os.makedirs('data/current', exist_ok=True)
    
    # Baseline compl√®te
    baseline = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'reference_date': reference_date,
            'date_range': f"{since_date} to {until_date}",
            'method': 'fetch_everything',
            'total_ads': len(enriched_ads),
            'accounts_processed': len(active_accounts),
            'accounts_success': len(active_accounts) - len(failed_accounts),
            'accounts_failed': len(failed_accounts),
            'has_demographics': True,
            'has_creatives': True
        },
        'ads': enriched_ads
    }
    
    with open('data/current/baseline_90d_complete.json', 'w', encoding='utf-8') as f:
        json.dump(baseline, f, indent=2, ensure_ascii=False)
    
    # 8. Cr√©er les p√©riodes agr√©g√©es
    print("\nüìä G√©n√©ration des p√©riodes...")
    
    for period in [3, 7, 14, 30, 90]:
        # Pour simplifier, on prend toutes les donn√©es
        # (normalement on filtrerait par date)
        
        total_spend = sum(float(ad.get('spend', 0)) for ad in enriched_ads)
        total_purchases = sum(int(ad.get('purchases', 0)) for ad in enriched_ads)
        total_value = sum(float(ad.get('purchase_value', 0)) for ad in enriched_ads)
        
        # Compter les ads actives
        active_ads = sum(1 for ad in enriched_ads if ad.get('effective_status') == 'ACTIVE')
        
        output = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'reference_date': reference_date,
                'period_days': period,
                'total_ads': len(enriched_ads),
                'active_ads': active_ads,
                'total_spend': total_spend,
                'total_purchases': total_purchases,
                'total_conversion_value': total_value,
                'avg_roas': total_value / total_spend if total_spend > 0 else 0,
                'avg_cpa': total_spend / total_purchases if total_purchases > 0 else 0,
                'has_demographics': True,
                'has_creatives': True
            },
            'ads': enriched_ads
        }
        
        with open(f'data/current/hybrid_data_{period}d.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"  ‚úÖ {period}j: {len(enriched_ads)} ads, {active_ads} actives, ${total_spend:,.0f} MXN")
    
    # 9. Semaine pr√©c√©dente (placeholder pour comparaison)
    prev_output = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'reference_date': (datetime.strptime(reference_date, '%Y-%m-%d') - timedelta(days=7)).strftime('%Y-%m-%d'),
            'period_days': 7,
            'total_ads': 0,
            'note': '√Ä impl√©menter: filtrer les donn√©es de la semaine pr√©c√©dente'
        },
        'ads': []
    }
    
    with open('data/current/hybrid_data_prev_week.json', 'w', encoding='utf-8') as f:
        json.dump(prev_output, f, indent=2, ensure_ascii=False)
    
    # 10. Config
    with open('data/current/refresh_config.json', 'w', encoding='utf-8') as f:
        json.dump({
            "last_update": datetime.now().isoformat(),
            "reference_date": reference_date,
            "periods_available": [3, 7, 14, 30, 90],
            "total_execution_time": time.time() - start_time,
            "accounts_processed": len(active_accounts),
            "accounts_success": len(active_accounts) - len(failed_accounts),
            "total_ads": len(enriched_ads),
            "active_ads": sum(1 for ad in enriched_ads if ad.get('effective_status') == 'ACTIVE'),
            "has_demographics": True,
            "has_creatives": True,
            "has_media_urls": True
        }, f, indent=2)
    
    print(f"\nüéâ TERMIN√â en {(time.time() - start_time)/60:.1f} minutes!")
    print(f"üíæ Tous les fichiers dans data/current/")
    print(f"‚ú® Donn√©es COMPL√àTES avec d√©mographies, status, format et media URLs!")
    
    if len(enriched_ads) > 0:
        print(f"\n‚ú® Succ√®s! Dashboard pr√™t avec {len(enriched_ads)} ads enrichies")
    else:
        print(f"\n‚ö†Ô∏è Aucune donn√©e r√©cup√©r√©e, v√©rifiez le token")

if __name__ == '__main__':
    main()