name: ü§ñ Auto Refresh Data

on:
  schedule:
    # Toutes les 2 heures (UTC times)
    # Mexico City est UTC-6, donc on ajuste
    - cron: '0 12,14,16,18,20,22,0,2,4 * * *'  # 6h,8h,10h,12h,14h,16h,18h,20h,22h Mexico
    # Nightly pour late attribution (7 jours)
    - cron: '0 3 * * *'  # 3am UTC = 9pm Mexico (une fois par nuit)
    
  workflow_dispatch:  # Permet de lancer manuellement depuis GitHub

jobs:
  refresh-data:
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    concurrency:
      group: pages-deploy    # m√™me groupe que deploy-fast pour s√©rialiser
      cancel-in-progress: false  # JAMAIS annuler pour √©viter les 404
    permissions:
      contents: write      # pour Release + checkout
      actions: read        # pour t√©l√©charger l'artefact Pages publi√©
      pages: write         # pour d√©ployer sur gh-pages
      id-token: write      # requis par deploy-pages
    
    steps:
    - name: üì• Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: üß∞ Install tooling
      run: sudo apt-get update && sudo apt-get install -y zstd jq unzip
    
    - name: üì• Restore baseline from GitHub Release
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        REL_TAG="baseline"
        ASSET="baseline_90d_daily.json.zst"
        mkdir -p data/current
        
        if gh release view "$REL_TAG" >/dev/null 2>&1; then
          echo "üîé Found release '$REL_TAG', downloading asset..."
          if gh release download "$REL_TAG" -p "$ASSET" -D data/current --clobber; then
            zstd -d -f "data/current/$ASSET" -o "data/current/baseline_90d_daily.json"
            echo "‚úÖ Baseline restored from Release ($(du -h data/current/baseline_90d_daily.json | cut -f1))"
            rm "data/current/$ASSET"  # Clean up compressed file
          else
            echo "‚ÑπÔ∏è Asset not found; will bootstrap."
            echo '{"daily_ads": [], "metadata": {}}' > data/current/baseline_90d_daily.json
          fi
        else
          echo "‚ÑπÔ∏è No release yet; will bootstrap."
          echo '{"daily_ads": [], "metadata": {}}' > data/current/baseline_90d_daily.json
        fi
    
    - name: üêç Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
    
    - name: üì¶ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests python-dotenv
    
    - name: üìä Run Daily Refresh
      env:
        FACEBOOK_ACCESS_TOKEN: ${{ secrets.FACEBOOK_ACCESS_TOKEN }}
        FRESHNESS_BUFFER_HOURS: 1
        # Alignement avec Facebook Ads Manager (Option A choisie par Martin)
        INCLUDE_TODAY: 0                # Exclure aujourd'hui (comme Ads Manager)
        ACTION_REPORT_TIME: conversion   # Compter √† la date de conversion
        USE_UNIFIED_ATTR: 1              # Attribution unifi√©e
        FILTER_IMPR_GT0: 0               # Ne PAS filtrer les 0 impressions
        # 7 jours pour couvrir les ads moins actives et √©viter les URLs expir√©es
        TAIL_BACKFILL_DAYS: 7
        RUN_BASELINE: 0
      timeout-minutes: 60
      run: |
        if [[ "${{ github.event.schedule }}" == "0 3 * * *" ]]; then
          echo "üåô Running nightly refresh (7 days backfill)..."
        else
          echo "üöÄ Running daily refresh (7 days backfill)..."
        fi
        python3 scripts/production/fetch_with_smart_limits.py
    
    - name: üßπ Prune baseline to 90 days
      run: |
        python3 - << 'PY'
        import json
        from datetime import datetime, timedelta
        
        p = 'data/current/baseline_90d_daily.json'
        try:
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
        except FileNotFoundError:
            exit(0)
        
        cutoff = (datetime.utcnow() - timedelta(days=89)).strftime('%Y-%m-%d')
        ads = [ad for ad in d.get('daily_ads', []) if ad.get('date', '') >= cutoff]
        old_count = len(d.get('daily_ads', []))
        d['daily_ads'] = ads
        
        with open(p, 'w', encoding='utf-8') as f:
            json.dump(d, f, ensure_ascii=False, separators=(',', ':'))
        
        print(f"‚úÇÔ∏è Pruned: {old_count} ‚Üí {len(ads)} rows (keeping since {cutoff})")
        PY
    
    # Compression already done by fetch_with_smart_limits.py via compress_after_fetch.py
    
    - name: üì§ Upload baseline to GitHub Release
      if: success()
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        REL_TAG="baseline"
        ASSET="baseline_90d_daily.json.zst"
        
        # Compress with maximum ratio
        zstd -19 -f -o "$ASSET" "data/current/baseline_90d_daily.json"
        echo "üì¶ Compressed: $(du -h $ASSET | cut -f1)"
        
        # Upload or create release
        if gh release view "$REL_TAG" >/dev/null 2>&1; then
          gh release upload "$REL_TAG" "$ASSET" --clobber
          echo "‚úÖ Baseline asset replaced on Release '$REL_TAG'"
        else
          gh release create "$REL_TAG" "$ASSET" \
            --title "Data baseline (90d rolling window)" \
            --notes "Auto-updated every 2 hours with latest Meta Ads data. Compressed with zstd."
          echo "‚úÖ Baseline Release created and asset uploaded"
        fi
    
    - name: üìã Copy to dashboard
      if: success()
      run: |
        mkdir -p docs/data
        rm -rf docs/data/optimized
        mkdir -p docs/data/optimized
        cp data/optimized/*.json docs/data/optimized/ 2>/dev/null || echo "No optimized files to copy yet"
        # Ensure prev_week file exists (even if empty)
        if [ ! -f docs/data/optimized/prev_week_compressed.json ]; then
          echo '{"period":"prev_week","ads":[]}' > docs/data/optimized/prev_week_compressed.json
        fi
    
    # Merge with existing Pages site to preserve code while updating data
    - name: üì¶ R√©cup√©rer le dernier site Pages publi√© (base du merge)
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        mkdir -p _site_prev
        RUN_ID=$(gh run list --workflow ".github/workflows/refresh-data.yml" --json databaseId,conclusion -L 20 \
          | jq -r '[.[]|select(.conclusion=="success")][0].databaseId // empty')
        if [ -z "$RUN_ID" ]; then
          RUN_ID=$(gh run list --workflow ".github/workflows/deploy-fast.yml" --json databaseId,conclusion -L 20 \
            | jq -r '[.[]|select(.conclusion=="success")][0].databaseId // empty')
        fi
        if [ -n "$RUN_ID" ]; then
          gh run download "$RUN_ID" -n github-pages -D _site_prev
          if compgen -G "_site_prev/*.zip" > /dev/null; then
            unzip -q _site_prev/*.zip -d _site_prev/extracted
            rm -f _site_prev/*.zip
            shopt -s dotglob
            mv _site_prev/extracted/* _site_prev/ || true
          fi
        fi

    - name: üß± Construire le bundle final (pr√©server code, remplacer donn√©es)
      run: |
        set -e
        mkdir -p _site
        if [ -d "_site_prev" ] && [ -n "$(ls -A _site_prev 2>/dev/null)" ]; then
          cp -a _site_prev/. _site/
        else
          # Fallback premi√®re fois: on part du repo
          cp -a docs/. _site/
        fi
        mkdir -p _site/data/optimized
        cp -a docs/data/optimized/. _site/data/optimized/

    - name: üìä Overlay demographics (repo ‚Üí _site)
      run: |
        if [ -d "docs/data/demographics" ]; then
          mkdir -p _site/data/demographics
          rsync -a docs/data/demographics/ _site/data/demographics/
          echo "‚úÖ Demographics copi√©s vers _site/"
        else
          echo "‚ÑπÔ∏è Aucun docs/data/demographics dans le repo"
        fi

    # Deploy to GitHub Pages instead of committing to master
    - name: üîé Sanity check (no empty site!)
      run: |
        test -s _site/index_full.html || (echo "‚ùå index_full.html is empty or missing!" && exit 1)
        test -s _site/data/optimized/meta_v1.json || (echo "‚ùå meta_v1.json is empty or missing!" && exit 1)
        echo "‚úÖ Site content verified"

    - name: üîß Setup Pages
      if: success()
      uses: actions/configure-pages@v5
    
    - name: üì¶ Upload Pages artifact
      if: success()
      uses: actions/upload-pages-artifact@v3
      with:
        path: _site
    
    - name: üöÄ Deploy to GitHub Pages
      if: success()
      id: deployment
      uses: actions/deploy-pages@v4
    
    - name: üìä Report status
      if: always()
      run: |
        echo "## üìä Refresh Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Mode**: ${{ github.event.schedule == '0 3 * * *' && 'Nightly (7 days)' || 'Daily (1 day)' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Time**: $(date '+%Y-%m-%d %H:%M:%S') UTC" >> $GITHUB_STEP_SUMMARY
        
        if [ -f data/current/baseline_90d_daily.json ]; then
          size=$(du -h data/current/baseline_90d_daily.json | cut -f1)
          count=$(python3 -c "import json; print(len(json.load(open('data/current/baseline_90d_daily.json'))['daily_ads']))" 2>/dev/null || echo "?")
          echo "- **Baseline**: $size, $count records" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f baseline_90d_daily.json.zst ]; then
          zsize=$(du -h baseline_90d_daily.json.zst | cut -f1)
          echo "- **Compressed**: $zsize" >> $GITHUB_STEP_SUMMARY
        fi