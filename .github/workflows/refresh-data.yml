name: ğŸ¤– Auto Refresh Data

on:
  schedule:
    # Toutes les 2 heures (UTC times)
    # Mexico City est UTC-6, donc on ajuste
    - cron: '0 12,14,16,18,20,22,0,2,4 * * *'  # 6h,8h,10h,12h,14h,16h,18h,20h,22h Mexico
    # Nightly pour late attribution (7 jours)
    - cron: '0 3 * * *'  # 3am UTC = 9pm Mexico (une fois par nuit)
    
  workflow_dispatch:  # Permet de lancer manuellement depuis GitHub

jobs:
  refresh-data:
    runs-on: ubuntu-latest
    concurrency:
      group: refresh-data
      cancel-in-progress: true
    permissions:
      contents: write  # Allow push to repository
    
    steps:
    - name: ğŸ“¥ Checkout repository
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: ğŸ§° Install zstd
      run: sudo apt-get update && sudo apt-get install -y zstd
    
    - name: ğŸ“¥ Restore baseline from GitHub Release
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        REL_TAG="baseline"
        ASSET="baseline_90d_daily.json.zst"
        mkdir -p data/current
        
        if gh release view "$REL_TAG" >/dev/null 2>&1; then
          echo "ğŸ” Found release '$REL_TAG', downloading asset..."
          if gh release download "$REL_TAG" -p "$ASSET" -D data/current --clobber; then
            zstd -d -f "data/current/$ASSET" -o "data/current/baseline_90d_daily.json"
            echo "âœ… Baseline restored from Release ($(du -h data/current/baseline_90d_daily.json | cut -f1))"
            rm "data/current/$ASSET"  # Clean up compressed file
          else
            echo "â„¹ï¸ Asset not found; will bootstrap."
            echo '{"daily_ads": [], "metadata": {}}' > data/current/baseline_90d_daily.json
          fi
        else
          echo "â„¹ï¸ No release yet; will bootstrap."
          echo '{"daily_ads": [], "metadata": {}}' > data/current/baseline_90d_daily.json
        fi
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests python-dotenv
    
    - name: ğŸ“Š Run Daily Refresh
      env:
        FACEBOOK_ACCESS_TOKEN: ${{ secrets.FACEBOOK_ACCESS_TOKEN }}
        FRESHNESS_BUFFER_HOURS: 2
        # 1 jour le jour, 7 jours la nuit pour late attribution
        TAIL_BACKFILL_DAYS: ${{ github.event.schedule == '0 3 * * *' && '7' || '1' }}
        RUN_BASELINE: 0
      timeout-minutes: 30
      run: |
        if [[ "${{ github.event.schedule }}" == "0 3 * * *" ]]; then
          echo "ğŸŒ™ Running nightly refresh (7 days for late attribution)..."
        else
          echo "ğŸš€ Running daily refresh (1 day)..."
        fi
        python3 scripts/production/fetch_with_smart_limits.py
    
    - name: ğŸ§¹ Prune baseline to 90 days
      run: |
        python3 - << 'PY'
        import json
        from datetime import datetime, timedelta
        
        p = 'data/current/baseline_90d_daily.json'
        try:
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
        except FileNotFoundError:
            exit(0)
        
        cutoff = (datetime.utcnow() - timedelta(days=89)).strftime('%Y-%m-%d')
        ads = [ad for ad in d.get('daily_ads', []) if ad.get('date', '') >= cutoff]
        old_count = len(d.get('daily_ads', []))
        d['daily_ads'] = ads
        
        with open(p, 'w', encoding='utf-8') as f:
            json.dump(d, f, ensure_ascii=False, separators=(',', ':'))
        
        print(f"âœ‚ï¸ Pruned: {old_count} â†’ {len(ads)} rows (keeping since {cutoff})")
        PY
    
    - name: ğŸ—œï¸ Compress data
      if: success()
      run: |
        echo "Compressing data..."
        python3 scripts/transform_to_columnar.py
    
    - name: ğŸ“¤ Upload baseline to GitHub Release
      if: success()
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        REL_TAG="baseline"
        ASSET="baseline_90d_daily.json.zst"
        
        # Compress with maximum ratio
        zstd -19 -f -o "$ASSET" "data/current/baseline_90d_daily.json"
        echo "ğŸ“¦ Compressed: $(du -h $ASSET | cut -f1)"
        
        # Upload or create release
        if gh release view "$REL_TAG" >/dev/null 2>&1; then
          gh release upload "$REL_TAG" "$ASSET" --clobber
          echo "âœ… Baseline asset replaced on Release '$REL_TAG'"
        else
          gh release create "$REL_TAG" "$ASSET" \
            --title "Data baseline (90d rolling window)" \
            --notes "Auto-updated every 2 hours with latest Meta Ads data. Compressed with zstd."
          echo "âœ… Baseline Release created and asset uploaded"
        fi
    
    - name: ğŸ“‹ Copy to dashboard
      if: success()
      run: |
        mkdir -p docs/data
        rm -rf docs/data/optimized
        mkdir -p docs/data/optimized
        cp data/optimized/*.json docs/data/optimized/ 2>/dev/null || echo "No optimized files to copy yet"
        # Ensure prev_week file exists (even if empty)
        if [ ! -f docs/data/optimized/prev_week_compressed.json ]; then
          echo '{"period":"prev_week","ads":[]}' > docs/data/optimized/prev_week_compressed.json
        fi
    
    - name: ğŸ“¤ Commit and push changes
      if: success()
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"
        
        git add -f docs/data/optimized/*.json
        # Also add prev_week if it exists
        git add -f docs/data/optimized/prev_week_compressed.json 2>/dev/null || true
        
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          MODE="${{ github.event.schedule == '0 3 * * *' && 'nightly 7d' || 'daily 1d' }}"
          git commit -m "ğŸ¤– Auto-refresh: $(date '+%Y-%m-%d %H:%M') UTC ($MODE)"
          
          # Pull before push to handle conflicts
          git pull --rebase origin master || git pull --no-rebase origin master
          
          # Push with retry logic
          for i in 1 2 3; do
            git push && break
            echo "Push attempt $i failed, retrying..."
            sleep 2
            git pull --rebase origin master || git pull --no-rebase origin master
          done
        fi
    
    - name: ğŸ“Š Report status
      if: always()
      run: |
        echo "## ğŸ“Š Refresh Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Mode**: ${{ github.event.schedule == '0 3 * * *' && 'Nightly (7 days)' || 'Daily (1 day)' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Time**: $(date '+%Y-%m-%d %H:%M:%S') UTC" >> $GITHUB_STEP_SUMMARY
        
        if [ -f data/current/baseline_90d_daily.json ]; then
          size=$(du -h data/current/baseline_90d_daily.json | cut -f1)
          count=$(python3 -c "import json; print(len(json.load(open('data/current/baseline_90d_daily.json'))['daily_ads']))" 2>/dev/null || echo "?")
          echo "- **Baseline**: $size, $count records" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f baseline_90d_daily.json.zst ]; then
          zsize=$(du -h baseline_90d_daily.json.zst | cut -f1)
          echo "- **Compressed**: $zsize" >> $GITHUB_STEP_SUMMARY
        fi