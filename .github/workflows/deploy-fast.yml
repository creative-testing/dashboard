name: ðŸš€ Fast Deploy (Code Only)

on:
  push:
    branches: [ master ]
    paths:
      - 'docs/**/*.html'
      - 'docs/**/*.js'
      - 'docs/**/*.css'
  workflow_dispatch:

permissions:
  contents: read
  actions: read          # ðŸ‘ˆ nÃ©cessaire pour tÃ©lÃ©charger l'artefact Pages publiÃ©
  pages: write
  id-token: write

concurrency:
  group: pages-deploy    # ðŸ‘ˆ sÃ©rialise UNIQUEMENT la phase de dÃ©ploiement
  cancel-in-progress: true

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - name: ðŸ“¥ Checkout repo (master)
        uses: actions/checkout@v4

      - name: ðŸ“¦ RÃ©cupÃ©rer le dernier site Pages publiÃ© (si disponible)
        run: |
          set -e
          mkdir -p _site_prev
          get_latest_run() {
            # Cherche d'abord un run rÃ©ussi du refresh-data (le plus courant)
            gh run list --workflow ".github/workflows/refresh-data.yml" --json databaseId,conclusion -L 20 \
              | jq -r '[.[]|select(.conclusion=="success")][0].databaseId // empty'
          }
          RUN_ID="$(get_latest_run)"
          if [ -z "$RUN_ID" ]; then
            # Fallback: un run rÃ©ussi du fast-deploy
            RUN_ID=$(gh run list --workflow ".github/workflows/deploy-fast.yml" --json databaseId,conclusion -L 20 \
              | jq -r '[.[]|select(.conclusion=="success")][0].databaseId // empty')
          fi

          if [ -n "$RUN_ID" ]; then
            echo "â„¹ï¸ TÃ©lÃ©chargement artefact Pages du run $RUN_IDâ€¦"
            gh run download "$RUN_ID" -n github-pages -D _site_prev
            # DÃ©zip si besoin
            if compgen -G "_site_prev/*.zip" > /dev/null; then
              unzip -q _site_prev/*.zip -d _site_prev/extracted
              rm -f _site_prev/*.zip
              shopt -s dotglob
              mv _site_prev/extracted/* _site_prev/ || true
            fi
          else
            echo "âš ï¸ Aucun artefact prÃ©cÃ©dent trouvÃ© (premier dÃ©ploiement ?)"
          fi

      - name: ðŸ§± Construire le bundle final (prÃ©server data/, mettre Ã  jour le code)
        run: |
          set -e
          mkdir -p _site
          # Base = dernier site publiÃ© si dispo, sinon l'Ã©tat du repo
          if [ -d "_site_prev" ] && [ -n "$(ls -A _site_prev 2>/dev/null)" ]; then
            cp -a _site_prev/. _site/
          else
            cp -a docs/. _site/
          fi
          # Met Ã  jour uniquement HTML/JS/CSS depuis le repo (ET NE TOUCHE PAS aux data optimisÃ©es)
          rsync -a \
            --include="*/" \
            --include="**/*.html" --include="**/*.js" --include="**/*.css" \
            --exclude="docs/data/optimized/**" \
            --exclude="*" \
            docs/ _site/

      - name: ðŸ§ª Valider/Bootstrapper les donnÃ©es (baseline â†’ transform â†’ fallback vide)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -e

          have_data() {
            [ -s "_site/data/optimized/meta_v1.json" ] && [ -s "_site/data/optimized/agg_v1.json" ]
          }

          # 0) Si dÃ©jÃ  OK (artefact Pages prÃ©cÃ©dent), on sort
          if have_data; then
            echo "âœ… DonnÃ©es optimisÃ©es dÃ©jÃ  prÃ©sentes"
            exit 0
          fi

          echo "â„¹ï¸ Pas de donnÃ©es optimisÃ©es dans l'artefact. Tentative de bootstrapâ€¦"
          mkdir -p data/current data/optimized

          # 1) Essayer de reconstruire depuis le Release 'baseline'
          if gh release view "baseline" >/dev/null 2>&1; then
            echo "â¬‡ï¸ TÃ©lÃ©chargement baseline_90d_daily.json.zst depuis Releaseâ€¦"
            if gh release download "baseline" -p "baseline_90d_daily.json.zst" -D data/current --clobber; then
              echo "ðŸ§° Install zstd"
              sudo apt-get update >/dev/null 2>&1
              sudo apt-get install -y zstd >/dev/null 2>&1
              echo "ðŸ—œï¸ DÃ©compression baselineâ€¦"
              zstd -d -f "data/current/baseline_90d_daily.json.zst" -o "data/current/baseline_90d_daily.json"

              echo "ðŸ Setup Python & transform columnar"
              python -V || true
              echo "::group::Setup Python"
              pip --version || true
              echo "::endgroup::"
            else
              echo "âš ï¸ TÃ©lÃ©chargement de la baseline impossible."
            fi
          else
            echo "â„¹ï¸ Aucun Release 'baseline' trouvÃ©."
          fi

          # 2) Si on a la baseline dÃ©compressÃ©e, produire data/optimized
          if [ -s "data/current/baseline_90d_daily.json" ]; then
            echo "ðŸ”„ Transformation vers format optimisÃ©â€¦"
            python3 scripts/transform_to_columnar.py --input-dir data/current --output-dir data/optimized
          fi

          # 3) Fallback final : gÃ©nÃ©rer des fichiers *vides mais valides* (schema-valid)
          if [ ! -s "data/optimized/meta_v1.json" ] || [ ! -s "data/optimized/agg_v1.json" ]; then
            echo "ðŸ§ª Fallback: gÃ©nÃ©ration de fichiers optimisÃ©s vides (schema-valid)â€¦"
            cat > /tmp/generate_empty.py << 'EOF'
import json, datetime, os
os.makedirs("data/optimized", exist_ok=True)
periods = ["3d","7d","14d","30d","90d"]
now = datetime.datetime.utcnow()
ref_date = (now - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
meta = {
  "version":1,
  "metadata":{
    "reference_date": ref_date,
    "reference_hour": now.strftime("%Y-%m-%d %H:%M:%S"),
    "buffer_hours": 0,
    "includes_today": False,
    "data_min_date": ref_date,
    "data_max_date": ref_date,
    "data_range_days": 1,
    "last_update": now.isoformat(),
    "source":"bootstrap_empty",
    "pipeline":"bootstrap"
  },
  "ads":[],
  "campaigns":{},
  "adsets":{},
  "accounts":{}
}
agg = {
  "version":1,
  "periods": periods,
  "metrics": ["impressions","clicks","purchases","spend","purchase_value","reach"],
  "ads": [],
  "values": [],
  "scales": {"money":100}
}
summary = {
  "periods": periods,
  "totals": {p: {"impr":0,"clk":0,"purch":0,"spend_cents":0,"purchase_value_cents":0,"reach":0} for p in periods}
}
manifest = {
  "version": now.isoformat(),
  "ads_count": 0,
  "periods": periods,
  "shards": {
    "meta": {"path":"meta_v1.json"},
    "agg": {"path":"agg_v1.json"},
    "summary": {"path":"summary_v1.json"}
  }
}
prevw = {"period":"prev_week","ads":[]}
for name, data in [
  ("meta_v1.json", meta),
  ("agg_v1.json", agg),
  ("summary_v1.json", summary),
  ("manifest.json", manifest),
  ("prev_week_compressed.json", prevw),
]:
  with open(os.path.join("data/optimized", name), "w", encoding="utf-8") as f:
    json.dump(data, f, separators=(",",":"), ensure_ascii=False)
EOF
            python3 /tmp/generate_empty.py
          fi

          # 4) Copier les data optimisÃ©es dans le bundle final
          mkdir -p _site/data/optimized
          cp -a data/optimized/. _site/data/optimized/

          # 5) Dernier garde-fou : vÃ©rifier la prÃ©sence
          if [ ! -s "_site/data/optimized/meta_v1.json" ] || [ ! -s "_site/data/optimized/agg_v1.json" ]; then
            echo "âŒ Impossible de prÃ©parer des donnÃ©es valides pour le dÃ©ploiement."
            exit 1
          fi

          echo "âœ… DonnÃ©es prÃªtes (baseline/empty fallback)."
          ls -la _site/data/optimized || true

      - name: ðŸ”§ Setup Pages
        uses: actions/configure-pages@v5

      - name: ðŸ“¦ Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: _site

      - name: ðŸš€ Deploy to Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: âœ… Summary
        run: |
          echo "## ðŸš€ Fast Deploy Complete" >> $GITHUB_STEP_SUMMARY
          echo "- URL: ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY